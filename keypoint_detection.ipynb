{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial Keypoint Detection\n",
    "This notebook implements facial keypoint detection (68 landmarks) using four approaches:\n",
    "1. **CNN** — Custom convolutional network (regression)\n",
    "2. **ResNet** — Transfer learning with ResNet18 (regression)\n",
    "3. **DINO** — Transfer learning with DINO Vision Transformer (regression)\n",
    "4. **U-Net** — Heatmap-based keypoint detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[69209]: Class AVFFrameReceiver is implemented in both /Users/mersadabbasi/anaconda3/lib/python3.13/site-packages/cv2/.dylibs/libavdevice.61.3.100.dylib (0x104f5c3a8) and /Users/mersadabbasi/anaconda3/lib/python3.13/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x121ce43a8). This may cause spurious casting failures and mysterious crashes. One of the duplicates must be removed or renamed.\n",
      "objc[69209]: Class AVFAudioReceiver is implemented in both /Users/mersadabbasi/anaconda3/lib/python3.13/site-packages/cv2/.dylibs/libavdevice.61.3.100.dylib (0x104f5c3f8) and /Users/mersadabbasi/anaconda3/lib/python3.13/site-packages/av/.dylibs/libavdevice.61.3.100.dylib (0x121ce43f8). This may cause spurious casting failures and mysterious crashes. One of the duplicates must be removed or renamed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import subprocess\n",
    "import zipfile\n",
    "\n",
    "import cv2\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from PIL import Image\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import models, transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data already exists.\n"
     ]
    }
   ],
   "source": [
    "def download_data(data_dir='data'):\n",
    "    if os.path.exists(os.path.join(data_dir, 'training')) and os.path.exists(os.path.join(data_dir, 'test')):\n",
    "        print(\"Data already exists.\")\n",
    "        return\n",
    "    print(\"Data not found. Downloading...\")\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    zip_path = os.path.join(data_dir, 'train-test-data.zip')\n",
    "    subprocess.run([\n",
    "        'wget', '-q', '-O', zip_path,\n",
    "        'https://s3.amazonaws.com/video.udacity-data.com/topher/2018/May/5aea1b91_train-test-data/train-test-data.zip'\n",
    "    ], check=True)\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall(data_dir)\n",
    "    os.remove(zip_path)\n",
    "    print(\"Data downloaded and extracted.\")\n",
    "\n",
    "download_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Rescale:\n",
    "    \"\"\"Rescale the image in a sample to a given size.\"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        h, w = image.shape[:2]\n",
    "        if isinstance(self.output_size, int):\n",
    "            if h > w:\n",
    "                new_h, new_w = self.output_size * h / w, self.output_size\n",
    "            else:\n",
    "                new_h, new_w = self.output_size, self.output_size * w / h\n",
    "        else:\n",
    "            new_h, new_w = self.output_size\n",
    "        new_h, new_w = int(new_h), int(new_w)\n",
    "        img = cv2.resize(image, (new_w, new_h))\n",
    "        key_pts = key_pts * [new_w / w, new_h / h]\n",
    "        return {'image': img, 'keypoints': key_pts}\n",
    "\n",
    "\n",
    "class RandomCrop:\n",
    "    \"\"\"Crop randomly the image in a sample.\"\"\"\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, (int, tuple))\n",
    "        if isinstance(output_size, int):\n",
    "            self.output_size = (output_size, output_size)\n",
    "        else:\n",
    "            self.output_size = output_size\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        h, w = image.shape[:2]\n",
    "        new_h, new_w = self.output_size\n",
    "        top = np.random.randint(0, h - new_h)\n",
    "        left = np.random.randint(0, w - new_w)\n",
    "        image = image[top:top + new_h, left:left + new_w]\n",
    "        key_pts = key_pts - [left, top]\n",
    "        return {'image': image, 'keypoints': key_pts}\n",
    "\n",
    "\n",
    "class NormalizeOriginal:\n",
    "    \"\"\"Convert to grayscale and normalize image [0,1], keypoints (pts-100)/50.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        image_copy = cv2.cvtColor(np.copy(image), cv2.COLOR_RGB2GRAY)\n",
    "        image_copy = (image_copy / 255.0).astype(np.float32)\n",
    "        key_pts_copy = (np.copy(key_pts) - 100) / 50.0\n",
    "        return {'image': image_copy, 'keypoints': key_pts_copy}\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        if len(image.shape) == 2:\n",
    "            image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "        image = image.transpose((2, 0, 1))\n",
    "        return {'image': torch.from_numpy(image), 'keypoints': torch.from_numpy(key_pts)}\n",
    "\n",
    "\n",
    "class RandomHorizontalFlip:\n",
    "    \"\"\"Random horizontal flip with correct keypoint mirroring for 68 landmarks.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        image_copy = np.copy(image)\n",
    "        key_pts_copy = np.copy(key_pts)\n",
    "        key_pts_copy_2 = np.copy(key_pts_copy)\n",
    "\n",
    "        if random.choice([0, 1]) <= 0.5:\n",
    "            image_copy = np.fliplr(image_copy)\n",
    "            key_pts_copy[:, 0] = -key_pts_copy[:, 0]\n",
    "            key_pts_copy[:, 0] = key_pts_copy[:, 0] + image_copy.shape[1]\n",
    "            key_pts_copy_2 = np.copy(key_pts_copy)\n",
    "\n",
    "            # Mirror jawline\n",
    "            for i, j in zip(range(0, 8), range(16, 8, -1)):\n",
    "                key_pts_copy_2[j] = key_pts_copy[i]\n",
    "                key_pts_copy_2[i] = key_pts_copy[j]\n",
    "            # Mirror eyebrows\n",
    "            for i, j in zip(range(17, 22), range(26, 21, -1)):\n",
    "                key_pts_copy_2[j] = key_pts_copy[i]\n",
    "                key_pts_copy_2[i] = key_pts_copy[j]\n",
    "            # Mirror nose tip\n",
    "            key_pts_copy_2[35] = key_pts_copy[31]\n",
    "            key_pts_copy_2[34] = key_pts_copy[32]\n",
    "            key_pts_copy_2[32] = key_pts_copy[34]\n",
    "            key_pts_copy_2[31] = key_pts_copy[35]\n",
    "            # Mirror eyes\n",
    "            key_pts_copy_2[45] = key_pts_copy[36]; key_pts_copy_2[44] = key_pts_copy[37]\n",
    "            key_pts_copy_2[43] = key_pts_copy[38]; key_pts_copy_2[42] = key_pts_copy[39]\n",
    "            key_pts_copy_2[47] = key_pts_copy[40]; key_pts_copy_2[46] = key_pts_copy[41]\n",
    "            key_pts_copy_2[39] = key_pts_copy[42]; key_pts_copy_2[38] = key_pts_copy[43]\n",
    "            key_pts_copy_2[37] = key_pts_copy[44]; key_pts_copy_2[36] = key_pts_copy[45]\n",
    "            key_pts_copy_2[41] = key_pts_copy[46]; key_pts_copy_2[40] = key_pts_copy[47]\n",
    "            # Mirror lips\n",
    "            key_pts_copy_2[54] = key_pts_copy[48]; key_pts_copy_2[53] = key_pts_copy[49]\n",
    "            key_pts_copy_2[52] = key_pts_copy[50]; key_pts_copy_2[50] = key_pts_copy[52]\n",
    "            key_pts_copy_2[49] = key_pts_copy[53]; key_pts_copy_2[48] = key_pts_copy[54]\n",
    "            key_pts_copy_2[59] = key_pts_copy[55]; key_pts_copy_2[58] = key_pts_copy[56]\n",
    "            key_pts_copy_2[56] = key_pts_copy[58]; key_pts_copy_2[55] = key_pts_copy[59]\n",
    "            key_pts_copy_2[64] = key_pts_copy[60]; key_pts_copy_2[63] = key_pts_copy[61]\n",
    "            key_pts_copy_2[61] = key_pts_copy[63]; key_pts_copy_2[60] = key_pts_copy[64]\n",
    "            key_pts_copy_2[67] = key_pts_copy[65]; key_pts_copy_2[65] = key_pts_copy[67]\n",
    "\n",
    "        return {'image': image_copy, 'keypoints': key_pts_copy_2}\n",
    "\n",
    "\n",
    "class RandomRotate:\n",
    "    \"\"\"Random rotation by +/- angle degrees.\"\"\"\n",
    "    def __init__(self, rotation=30):\n",
    "        self.rotation = rotation\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        image_copy = np.copy(image)\n",
    "        key_pts_copy = np.copy(key_pts)\n",
    "        rows, cols = image.shape[:2]\n",
    "        M = cv2.getRotationMatrix2D((rows / 2, cols / 2), random.choice([-self.rotation, self.rotation]), 1)\n",
    "        image_copy = cv2.warpAffine(image_copy, M, (cols, rows))\n",
    "        key_pts_copy = key_pts_copy.reshape((1, 136))\n",
    "        new_keypoints = np.zeros(136)\n",
    "        for i in range(68):\n",
    "            coord_idx = 2 * i\n",
    "            old_coord = key_pts_copy[0][coord_idx:coord_idx + 2]\n",
    "            new_coord = np.matmul(M, np.append(old_coord, 1))\n",
    "            new_keypoints[coord_idx] += new_coord[0]\n",
    "            new_keypoints[coord_idx + 1] += new_coord[1]\n",
    "        return {'image': image_copy, 'keypoints': new_keypoints.reshape((68, 2))}\n",
    "\n",
    "\n",
    "class ColorJitter:\n",
    "    \"\"\"Random color jitter (brightness, contrast, saturation).\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        image, key_pts = sample['image'], sample['keypoints']\n",
    "        color_jitter = transforms.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4)\n",
    "        image_copy = np.copy(image)\n",
    "        if image_copy.dtype != np.uint8:\n",
    "            image_copy = (image_copy * 255).astype(np.uint8)\n",
    "        image_copy = np.array(color_jitter(Image.fromarray(image_copy)))\n",
    "        return {'image': image_copy, 'keypoints': np.copy(key_pts)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FacialKeypointsDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset for regression-based models.\"\"\"\n",
    "    def __init__(self, csv_file, root_dir, transform=None):\n",
    "        self.key_pts_frame = pd.read_csv(csv_file, index_col=0)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_pts_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = os.path.join(self.root_dir, self.key_pts_frame.index[idx])\n",
    "        image = mpimg.imread(image_name)\n",
    "        if image.shape[2] == 4:\n",
    "            image = image[:, :, 0:3]\n",
    "        key_pts = self.key_pts_frame.iloc[idx, :].values.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'keypoints': key_pts}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "\n",
    "class FacialKeypointsHeatmapDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset with heatmap generation for U-Net.\"\"\"\n",
    "    def __init__(self, csv_file, root_dir, transform=None, output_size=64, sigma=1, image_size=224):\n",
    "        self.key_pts_frame = pd.read_csv(csv_file, index_col=0)\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.output_size = output_size\n",
    "        self.sigma = sigma\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.key_pts_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = os.path.join(self.root_dir, self.key_pts_frame.index[idx])\n",
    "        image = mpimg.imread(image_name)\n",
    "        if image.shape[2] == 4:\n",
    "            image = image[:, :, 0:3]\n",
    "        key_pts = self.key_pts_frame.iloc[idx, :].values.astype('float').reshape(-1, 2)\n",
    "        sample = {'image': image, 'keypoints': key_pts}\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        sample['heatmaps'] = self.generate_heatmaps(sample['keypoints'])\n",
    "        return sample\n",
    "\n",
    "    def generate_heatmaps(self, keypoints):\n",
    "        if isinstance(keypoints, torch.Tensor):\n",
    "            keypoints = keypoints.numpy()\n",
    "        num_keypoints = keypoints.shape[0]\n",
    "        heatmaps = np.zeros((num_keypoints, self.output_size, self.output_size), dtype=np.float32)\n",
    "        keypoints_scaled = (keypoints * 50 + 100) * (self.output_size / self.image_size)\n",
    "        for i in range(num_keypoints):\n",
    "            x, y = keypoints_scaled[i]\n",
    "            if np.isnan(x) or np.isnan(y):\n",
    "                continue\n",
    "            x_int = max(0, min(self.output_size - 1, int(x)))\n",
    "            y_int = max(0, min(self.output_size - 1, int(y)))\n",
    "            heatmap = np.zeros((self.output_size, self.output_size), dtype=np.float32)\n",
    "            heatmap[y_int, x_int] = 1.0\n",
    "            heatmap = gaussian_filter(heatmap, sigma=self.sigma)\n",
    "            if heatmap.max() > 0:\n",
    "                heatmap = heatmap / heatmap.max()\n",
    "            heatmaps[i] = heatmap\n",
    "        return torch.from_numpy(heatmaps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_regression_data(batch_size=64):\n",
    "    \"\"\"Load datasets and dataloaders for regression-based models (CNN, ResNet, DINO).\"\"\"\n",
    "    data_transform = transforms.Compose([\n",
    "        Rescale(250), RandomCrop(224), NormalizeOriginal(), ToTensor()\n",
    "    ])\n",
    "    train_dataset = FacialKeypointsDataset('data/training_frames_keypoints.csv', 'data/training', data_transform)\n",
    "    test_dataset = FacialKeypointsDataset('data/test_frames_keypoints.csv', 'data/test', data_transform)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    return train_loader, test_loader\n",
    "\n",
    "\n",
    "def load_heatmap_data(batch_size=256, heatmap_size=64):\n",
    "    \"\"\"Load datasets and dataloaders for heatmap-based models (U-Net).\"\"\"\n",
    "    train_transform = transforms.Compose([\n",
    "        Rescale(250), RandomCrop(224), RandomHorizontalFlip(), RandomRotate(15),\n",
    "        ColorJitter(), NormalizeOriginal(), ToTensor()\n",
    "    ])\n",
    "    test_transform = transforms.Compose([\n",
    "        Rescale((224, 224)), NormalizeOriginal(), ToTensor()\n",
    "    ])\n",
    "    train_dataset = FacialKeypointsHeatmapDataset(\n",
    "        'data/training_frames_keypoints.csv', 'data/training',\n",
    "        transform=train_transform, output_size=heatmap_size, sigma=2, image_size=224)\n",
    "    test_dataset = FacialKeypointsHeatmapDataset(\n",
    "        'data/test_frames_keypoints.csv', 'data/test',\n",
    "        transform=test_transform, output_size=heatmap_size, sigma=2, image_size=224)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Training & Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, epoch, step, model_name, path='checkpoints/last_checkpoint.pth'):\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'epoch': epoch, 'step': step, 'model_name': model_name,\n",
    "    }, path)\n",
    "    print(f\"Checkpoint saved to {path}\")\n",
    "\n",
    "\n",
    "def evaluate_regression(model, test_loader, criterion, device):\n",
    "    \"\"\"Evaluate a regression model (CNN/ResNet/DINO) on the test set.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            keypoints = batch['keypoints'].view(images.size(0), -1).to(device)\n",
    "            outputs = model(images)\n",
    "            loss = model.compute_loss(outputs, keypoints, criterion)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "\n",
    "def evaluate_heatmap(model, test_loader, device, loss_type='mse'):\n",
    "    \"\"\"Evaluate U-Net on the test set.\"\"\"\n",
    "    model.eval()\n",
    "    if loss_type == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10.0]).to(device))\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            images = batch['image'].to(device)\n",
    "            target = batch['heatmaps'].to(device)\n",
    "            logits = model(images)\n",
    "            if loss_type == 'mse':\n",
    "                total_loss += criterion(torch.sigmoid(logits), target).item()\n",
    "            else:\n",
    "                total_loss += criterion(logits, target).item()\n",
    "    return total_loss / len(test_loader)\n",
    "\n",
    "\n",
    "def heatmaps_to_keypoints(heatmaps, heatmap_size=64, image_size=224):\n",
    "    \"\"\"Extract keypoint coordinates from heatmaps using argmax.\"\"\"\n",
    "    batch_size, num_kpts, h, w = heatmaps.shape\n",
    "    heatmaps_flat = heatmaps.view(batch_size, num_kpts, -1)\n",
    "    max_indices = heatmaps_flat.argmax(dim=2)\n",
    "    y_coords = (max_indices // w).float()\n",
    "    x_coords = (max_indices % w).float()\n",
    "    x_coords = x_coords * (image_size / heatmap_size)\n",
    "    y_coords = y_coords * (image_size / heatmap_size)\n",
    "    x_norm = (x_coords - 100) / 50.0\n",
    "    y_norm = (y_coords - 100) / 50.0\n",
    "    return torch.stack([x_norm, y_norm], dim=2)\n",
    "\n",
    "\n",
    "def train_regression(model, train_loader, test_loader, optimizer, criterion, device,\n",
    "                     model_name='model', num_epochs=10, eval_interval=10,\n",
    "                     log_interval=5, save_interval=30):\n",
    "    \"\"\"Training loop for regression-based models (CNN, ResNet, DINO).\"\"\"\n",
    "    step = 0\n",
    "    running_loss = 0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            step += 1\n",
    "            image = batch['image'].to(device)\n",
    "            keypoints = batch['keypoints'].reshape(image.size(0), 136).float().to(device)\n",
    "\n",
    "            preds = model(image)\n",
    "            loss = model.compute_loss(preds, keypoints, criterion)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % log_interval == 0:\n",
    "                avg_loss = running_loss / log_interval\n",
    "                wandb.log({'train_loss': avg_loss, 'step': step})\n",
    "                print(f\"Step {step}: Train Loss = {avg_loss:.4f}\")\n",
    "                running_loss = 0\n",
    "\n",
    "            if step % eval_interval == 0:\n",
    "                val_loss = evaluate_regression(model, test_loader, criterion, device)\n",
    "                wandb.log({'val_loss': val_loss, 'step': step})\n",
    "                model.train()\n",
    "\n",
    "            if step % save_interval == 0:\n",
    "                save_checkpoint(model, optimizer, epoch, step, model_name,\n",
    "                                path=f'checkpoints-{model_name}/step_{step}.pth')\n",
    "\n",
    "    print(f\"Training complete for {model_name}!\")\n",
    "    return epoch, step\n",
    "\n",
    "\n",
    "def train_heatmap(model, train_loader, test_loader, optimizer, device,\n",
    "                  model_name='unet', num_epochs=10, eval_interval=10,\n",
    "                  log_interval=5, save_interval=30, loss_type='mse',\n",
    "                  scheduler_type='cosine'):\n",
    "    \"\"\"Training loop for heatmap-based models (U-Net).\n",
    "    \n",
    "    Args:\n",
    "        loss_type: 'mse' (recommended) or 'bce'\n",
    "        scheduler_type: 'cosine' (recommended) or 'plateau'\n",
    "    \"\"\"\n",
    "    step = 0\n",
    "    running_loss = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    if loss_type == 'mse':\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([10.0]).to(device))\n",
    "\n",
    "    total_steps = num_epochs * len(train_loader)\n",
    "    if scheduler_type == 'cosine':\n",
    "        scheduler = lr_scheduler.CosineAnnealingLR(optimizer, T_max=total_steps, eta_min=1e-6)\n",
    "    else:\n",
    "        scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            step += 1\n",
    "            images = batch['image'].to(device)\n",
    "            heatmaps_gt = batch['heatmaps'].to(device)\n",
    "\n",
    "            logits = model(images)\n",
    "\n",
    "            if loss_type == 'mse':\n",
    "                loss = criterion(torch.sigmoid(logits), heatmaps_gt)\n",
    "            else:\n",
    "                loss = criterion(logits, heatmaps_gt)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if scheduler_type == 'cosine':\n",
    "                scheduler.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            if step % log_interval == 0:\n",
    "                avg_loss = running_loss / log_interval\n",
    "                wandb.log({'train_loss': avg_loss, 'step': step, 'epoch': epoch,\n",
    "                           'lr': optimizer.param_groups[0]['lr']})\n",
    "                print(f\"Epoch {epoch}, Step {step}: Train Loss = {avg_loss:.6f} (lr={optimizer.param_groups[0]['lr']:.6f})\")\n",
    "                running_loss = 0\n",
    "\n",
    "            if step % eval_interval == 0:\n",
    "                val_loss = evaluate_heatmap(model, test_loader, device, loss_type=loss_type)\n",
    "                wandb.log({'val_loss': val_loss, 'step': step, 'epoch': epoch})\n",
    "                print(f\"Epoch {epoch}, Step {step}: Val Loss = {val_loss:.6f}\")\n",
    "                model.train()\n",
    "                if val_loss < best_val_loss:\n",
    "                    best_val_loss = val_loss\n",
    "                    save_checkpoint(model, optimizer, epoch, step, model_name,\n",
    "                                    path=f'checkpoints-{model_name}/best.pth')\n",
    "                    print(f\"  -> New best val loss: {val_loss:.6f}\")\n",
    "\n",
    "            if step % save_interval == 0:\n",
    "                save_checkpoint(model, optimizer, epoch, step, model_name,\n",
    "                                path=f'checkpoints-{model_name}/step_{step}.pth')\n",
    "\n",
    "        # End of epoch\n",
    "        if scheduler_type == 'plateau':\n",
    "            epoch_val_loss = evaluate_heatmap(model, test_loader, device, loss_type=loss_type)\n",
    "            scheduler.step(epoch_val_loss)\n",
    "            print(f\"--- Epoch {epoch} done. Val Loss = {epoch_val_loss:.6f}, LR = {optimizer.param_groups[0]['lr']:.6f} ---\")\n",
    "        else:\n",
    "            print(f\"--- Epoch {epoch} done. LR = {optimizer.param_groups[0]['lr']:.6f} ---\")\n",
    "        model.train()\n",
    "\n",
    "    print(\"Heatmap training complete!\")\n",
    "    return epoch, step\n",
    "\n",
    "\n",
    "def visualize_keypoints(test_loader, model, device='cuda'):\n",
    "    \"\"\"Visualize predicted vs ground-truth keypoints on test samples.\"\"\"\n",
    "    model.eval()\n",
    "    for i, data in enumerate(test_loader):\n",
    "        image = data['image'][0]\n",
    "        images = data['image']\n",
    "        with torch.no_grad():\n",
    "            images = images.to(device)\n",
    "            predictions = model(images)\n",
    "        if predictions.dim() == 4:\n",
    "            predictions = heatmaps_to_keypoints(predictions)\n",
    "        else:\n",
    "            predictions = predictions.reshape(images.size(0), 68, 2)\n",
    "        predictions = predictions.cpu().numpy()\n",
    "        pred_kpts = predictions[0]\n",
    "        gt_kpts = data['keypoints'][0].numpy()\n",
    "        pred_kpts_denorm = (pred_kpts * 50) + 100\n",
    "        gt_kpts_denorm = (gt_kpts * 50) + 100\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.imshow(image.numpy().transpose(1, 2, 0), cmap='gray')\n",
    "        plt.scatter(pred_kpts_denorm[:, 0], pred_kpts_denorm[:, 1], c='r', s=20, label='Predicted')\n",
    "        plt.scatter(gt_kpts_denorm[:, 0], gt_kpts_denorm[:, 1], c='g', s=20, label='Ground Truth')\n",
    "        plt.legend()\n",
    "        plt.title(f'Test Sample {i}')\n",
    "        plt.show()\n",
    "        if i >= 4:\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_keypoints=68, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.fc1 = nn.Linear(256 * 14 * 14, 512)\n",
    "        self.fc2 = nn.Linear(512, num_keypoints * 2)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.bn1(self.conv1(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.bn2(self.conv2(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.bn3(self.conv3(x))), 2)\n",
    "        x = F.max_pool2d(F.relu(self.bn4(self.conv4(x))), 2)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout(F.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "    def compute_loss(self, preds, labels, criterion):\n",
    "        if criterion == 'mse':\n",
    "            loss_fn = nn.MSELoss()\n",
    "        elif criterion == 'Smoothl1Loss':\n",
    "            loss_fn = nn.SmoothL1Loss()\n",
    "        else:\n",
    "            raise ValueError(f'Unknown criterion: {criterion}')\n",
    "        return loss_fn(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Train CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "CNN_EPOCHS = 50\n",
    "CNN_CRITERION = 'Smoothl1Loss'  # or 'mse'\n",
    "CNN_LR = 1e-3\n",
    "CNN_BATCH_SIZE = 64\n",
    "\n",
    "# --- Initialize ---\n",
    "cnn_model = CNN().to(device)\n",
    "cnn_train_loader, cnn_test_loader = load_regression_data(batch_size=CNN_BATCH_SIZE)\n",
    "cnn_optimizer = torch.optim.Adam(cnn_model.parameters(), lr=CNN_LR)\n",
    "\n",
    "print(f\"CNN parameters: {sum(p.numel() for p in cnn_model.parameters()):,}\")\n",
    "\n",
    "# --- Train ---\n",
    "wandb.init(project='facial-keypoints', name='cnn-train', reinit=True)\n",
    "wandb.config.update({'model': 'cnn', 'criterion': CNN_CRITERION, 'lr': CNN_LR})\n",
    "\n",
    "cnn_epoch, cnn_step = train_regression(\n",
    "    cnn_model, cnn_train_loader, cnn_test_loader, cnn_optimizer,\n",
    "    CNN_CRITERION, device, model_name='cnn', num_epochs=CNN_EPOCHS\n",
    ")\n",
    "\n",
    "save_checkpoint(cnn_model, cnn_optimizer, cnn_epoch, cnn_step, 'cnn',\n",
    "                path='checkpoints-cnn/last_checkpoint.pth')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Visualize CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_keypoints(cnn_test_loader, cnn_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 ResNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetKeypointDetector(nn.Module):\n",
    "    def __init__(self, num_keypoints=68, backbone='resnet18', pretrained=True, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        if backbone == 'resnet18':\n",
    "            self.backbone = models.resnet18(pretrained=pretrained)\n",
    "        elif backbone == 'resnet34':\n",
    "            self.backbone = models.resnet34(pretrained=pretrained)\n",
    "\n",
    "        # Adapt first conv for grayscale\n",
    "        original_conv = self.backbone.conv1\n",
    "        self.backbone.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        if pretrained:\n",
    "            with torch.no_grad():\n",
    "                self.backbone.conv1.weight[:, 0, :, :] = original_conv.weight[:, 0, :, :]\n",
    "\n",
    "        self.backbone_out_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Identity()\n",
    "\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(self.backbone_out_features, 1024), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_keypoints * 2),\n",
    "        )\n",
    "\n",
    "        self._backbone_frozen = freeze_backbone\n",
    "        if freeze_backbone:\n",
    "            self._freeze_backbone()\n",
    "\n",
    "    def _freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._backbone_frozen = True\n",
    "\n",
    "    def _unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        self._backbone_frozen = False\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Override train() to keep frozen backbone in eval mode.\n",
    "        This prevents BatchNorm from updating running stats on frozen features.\"\"\"\n",
    "        super().train(mode)\n",
    "        if mode and self._backbone_frozen:\n",
    "            self.backbone.eval()\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.regression_head(features)\n",
    "\n",
    "    def compute_loss(self, preds, labels, criterion):\n",
    "        if criterion == 'mse':\n",
    "            loss_fn = nn.MSELoss()\n",
    "        elif criterion == 'Smoothl1Loss':\n",
    "            loss_fn = nn.SmoothL1Loss()\n",
    "        else:\n",
    "            raise ValueError(f'Unknown criterion: {criterion}')\n",
    "        return loss_fn(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Train ResNet (Stage 1: Frozen Backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "RESNET_EPOCHS_FROZEN = 50\n",
    "RESNET_CRITERION = 'Smoothl1Loss'\n",
    "RESNET_LR_FROZEN = 5e-4\n",
    "RESNET_BATCH_SIZE = 64\n",
    "\n",
    "# --- Initialize ---\n",
    "resnet_model = ResNetKeypointDetector(backbone='resnet18', pretrained=True, freeze_backbone=True).to(device)\n",
    "resnet_train_loader, resnet_test_loader = load_regression_data(batch_size=RESNET_BATCH_SIZE)\n",
    "resnet_optimizer = torch.optim.Adam(\n",
    "    [p for p in resnet_model.parameters() if p.requires_grad], lr=RESNET_LR_FROZEN\n",
    ")\n",
    "\n",
    "print(f\"ResNet parameters (total): {sum(p.numel() for p in resnet_model.parameters()):,}\")\n",
    "print(f\"ResNet parameters (trainable): {sum(p.numel() for p in resnet_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# --- Train Stage 1: Frozen backbone ---\n",
    "wandb.init(project='facial-keypoints', name='resnet-frozen', reinit=True)\n",
    "wandb.config.update({'model': 'resnet', 'criterion': RESNET_CRITERION, 'freeze': True})\n",
    "\n",
    "resnet_epoch, resnet_step = train_regression(\n",
    "    resnet_model, resnet_train_loader, resnet_test_loader, resnet_optimizer,\n",
    "    RESNET_CRITERION, device, model_name='resnet', num_epochs=RESNET_EPOCHS_FROZEN\n",
    ")\n",
    "\n",
    "save_checkpoint(resnet_model, resnet_optimizer, resnet_epoch, resnet_step, 'resnet',\n",
    "                path='checkpoints-resnet/frozen_checkpoint.pth')\n",
    "wandb.finish()\n",
    "print(\"Stage 1 (frozen backbone) complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Train ResNet (Stage 2: Fine-tune Full Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "RESNET_EPOCHS_FINETUNE = 50\n",
    "RESNET_LR_FINETUNE = 1e-4\n",
    "\n",
    "# --- Unfreeze and set up new optimizer ---\n",
    "resnet_model._unfreeze_backbone()\n",
    "resnet_optimizer = torch.optim.Adam(resnet_model.parameters(), lr=RESNET_LR_FINETUNE)\n",
    "\n",
    "print(f\"ResNet parameters (trainable after unfreeze): {sum(p.numel() for p in resnet_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# --- Train Stage 2: Fine-tune ---\n",
    "wandb.init(project='facial-keypoints', name='resnet-finetune', reinit=True)\n",
    "wandb.config.update({'model': 'resnet', 'criterion': RESNET_CRITERION, 'freeze': False})\n",
    "\n",
    "resnet_epoch, resnet_step = train_regression(\n",
    "    resnet_model, resnet_train_loader, resnet_test_loader, resnet_optimizer,\n",
    "    RESNET_CRITERION, device, model_name='resnet', num_epochs=RESNET_EPOCHS_FINETUNE\n",
    ")\n",
    "\n",
    "save_checkpoint(resnet_model, resnet_optimizer, resnet_epoch, resnet_step, 'resnet',\n",
    "                path='checkpoints-resnet/last_checkpoint.pth')\n",
    "wandb.finish()\n",
    "print(\"Stage 2 (fine-tune) complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Visualize ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_keypoints(resnet_test_loader, resnet_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 DINO Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DINOKeypointDetector(nn.Module):\n",
    "    def __init__(self, num_keypoints=68, model_name='vit_base_patch16_224.dino',\n",
    "                 pretrained=True, freeze_backbone=True):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(model_name, pretrained=pretrained)\n",
    "        self.backbone_out_features = self.backbone.embed_dim\n",
    "        self.backbone.head = nn.Identity()\n",
    "\n",
    "        # ImageNet normalization (DINO was trained with this)\n",
    "        self.register_buffer('img_mean', torch.tensor([0.485, 0.456, 0.406]).view(1, 3, 1, 1))\n",
    "        self.register_buffer('img_std', torch.tensor([0.229, 0.224, 0.225]).view(1, 3, 1, 1))\n",
    "\n",
    "        # BatchNorm to amplify inter-sample feature differences\n",
    "        self.feature_norm = nn.BatchNorm1d(self.backbone_out_features)\n",
    "\n",
    "        self.regression_head = nn.Sequential(\n",
    "            nn.Linear(self.backbone_out_features, 1024), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_keypoints * 2),\n",
    "        )\n",
    "\n",
    "        self._backbone_frozen = freeze_backbone\n",
    "        if freeze_backbone:\n",
    "            self._freeze_backbone()\n",
    "\n",
    "    def _freeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = False\n",
    "        self._backbone_frozen = True\n",
    "\n",
    "    def _unfreeze_backbone(self):\n",
    "        for param in self.backbone.parameters():\n",
    "            param.requires_grad = True\n",
    "        self._backbone_frozen = False\n",
    "\n",
    "    def train(self, mode=True):\n",
    "        \"\"\"Keep frozen backbone in eval mode to disable DropPath.\"\"\"\n",
    "        super().train(mode)\n",
    "        if mode and self._backbone_frozen:\n",
    "            self.backbone.eval()\n",
    "        return self\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Grayscale → RGB\n",
    "        if x.size(1) == 1:\n",
    "            x = x.repeat(1, 3, 1, 1)\n",
    "\n",
    "        # Apply ImageNet normalization\n",
    "        x = (x - self.img_mean) / self.img_std\n",
    "\n",
    "        # Extract CLS token features (more discriminative than patch avg)\n",
    "        features = self.backbone.forward_features(x)\n",
    "        if len(features.shape) > 2:\n",
    "            features = features[:, 0, :]  # CLS token\n",
    "\n",
    "        # Normalize features — amplifies inter-sample differences\n",
    "        features = self.feature_norm(features)\n",
    "\n",
    "        return self.regression_head(features)\n",
    "\n",
    "    def compute_loss(self, preds, labels, criterion):\n",
    "        if criterion == 'mse':\n",
    "            loss_fn = nn.MSELoss()\n",
    "        elif criterion == 'Smoothl1Loss':\n",
    "            loss_fn = nn.SmoothL1Loss()\n",
    "        else:\n",
    "            raise ValueError(f'Unknown criterion: {criterion}')\n",
    "        return loss_fn(preds, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 Train DINO (Stage 1: Frozen Backbone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DINO_EPOCHS_FROZEN = 50\n",
    "DINO_CRITERION = 'Smoothl1Loss'\n",
    "DINO_LR_FROZEN = 5e-4\n",
    "DINO_BATCH_SIZE = 64\n",
    "\n",
    "# --- Initialize ---\n",
    "dino_model = DINOKeypointDetector(pretrained=True, freeze_backbone=True).to(device)\n",
    "dino_train_loader, dino_test_loader = load_regression_data(batch_size=DINO_BATCH_SIZE)\n",
    "dino_optimizer = torch.optim.Adam(\n",
    "    [p for p in dino_model.parameters() if p.requires_grad], lr=DINO_LR_FROZEN\n",
    ")\n",
    "\n",
    "print(f\"DINO parameters (total): {sum(p.numel() for p in dino_model.parameters()):,}\")\n",
    "print(f\"DINO parameters (trainable): {sum(p.numel() for p in dino_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# --- Train Stage 1: Frozen backbone ---\n",
    "wandb.init(project='facial-keypoints', name='dino-frozen', reinit=True)\n",
    "wandb.config.update({'model': 'dino', 'criterion': DINO_CRITERION, 'freeze': True})\n",
    "\n",
    "dino_epoch, dino_step = train_regression(\n",
    "    dino_model, dino_train_loader, dino_test_loader, dino_optimizer,\n",
    "    DINO_CRITERION, device, model_name='dino', num_epochs=DINO_EPOCHS_FROZEN\n",
    ")\n",
    "\n",
    "save_checkpoint(dino_model, dino_optimizer, dino_epoch, dino_step, 'dino',\n",
    "                path='checkpoints-dino/frozen_checkpoint.pth')\n",
    "wandb.finish()\n",
    "print(\"DINO Stage 1 (frozen backbone) complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 Train DINO (Stage 2: Fine-tune Full Model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DINO_EPOCHS_FINETUNE = 50\n",
    "DINO_LR_FINETUNE = 1e-4\n",
    "\n",
    "# --- Unfreeze and set up new optimizer ---\n",
    "dino_model._unfreeze_backbone()\n",
    "dino_optimizer = torch.optim.Adam(dino_model.parameters(), lr=DINO_LR_FINETUNE)\n",
    "\n",
    "print(f\"DINO parameters (trainable after unfreeze): {sum(p.numel() for p in dino_model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# --- Train Stage 2: Fine-tune ---\n",
    "wandb.init(project='facial-keypoints', name='dino-finetune', reinit=True)\n",
    "wandb.config.update({'model': 'dino', 'criterion': DINO_CRITERION, 'freeze': False})\n",
    "\n",
    "dino_epoch, dino_step = train_regression(\n",
    "    dino_model, dino_train_loader, dino_test_loader, dino_optimizer,\n",
    "    DINO_CRITERION, device, model_name='dino', num_epochs=DINO_EPOCHS_FINETUNE\n",
    ")\n",
    "\n",
    "save_checkpoint(dino_model, dino_optimizer, dino_epoch, dino_step, 'dino',\n",
    "                path='checkpoints-dino/last_checkpoint.pth')\n",
    "wandb.finish()\n",
    "print(\"DINO Stage 2 (fine-tune) complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.8 Visualize DINO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_keypoints(dino_test_loader, dino_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.9 DEBUG: DINO 32-Sample Overfit Test\n",
    "If the model can't overfit 32 samples, there's a bug in labels / transforms / optimizer / forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean-baseline MSE: 0.048850\n",
      "\n",
      "Step   1: MSE = 0.364251\n",
      "Step  50: MSE = 0.008731\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m losses \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m501\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m     preds \u001b[38;5;241m=\u001b[39m dino_overfit(images)\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(preds, keypoints)\n\u001b[1;32m     32\u001b[0m     opt\u001b[38;5;241m.\u001b[39mzero_grad(); loss\u001b[38;5;241m.\u001b[39mbackward(); opt\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[9], line 52\u001b[0m, in \u001b[0;36mDINOKeypointDetector.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     49\u001b[0m x \u001b[38;5;241m=\u001b[39m (x \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_mean) \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimg_std\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# Extract CLS token features (more discriminative than patch avg)\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackbone\u001b[38;5;241m.\u001b[39mforward_features(x)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(features\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m     54\u001b[0m     features \u001b[38;5;241m=\u001b[39m features[:, \u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# CLS token\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/timm/models/vision_transformer.py:1202\u001b[0m, in \u001b[0;36mVisionTransformer.forward_features\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m   1200\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m   1201\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1202\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks(x)\n\u001b[1;32m   1204\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[1;32m   1205\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/container.py:240\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 240\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m module(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/timm/models/vision_transformer.py:205\u001b[0m, in \u001b[0;36mBlock.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor, attn_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 205\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm1(x), attn_mask\u001b[38;5;241m=\u001b[39mattn_mask)))\n\u001b[1;32m    206\u001b[0m     x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop_path2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mls2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x))))\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/timm/layers/attention.py:89\u001b[0m, in \u001b[0;36mAttention.forward\u001b[0;34m(self, x, attn_mask)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     85\u001b[0m         x: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[1;32m     86\u001b[0m         attn_mask: Optional[torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     87\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m     88\u001b[0m     B, N, C \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 89\u001b[0m     qkv \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mqkv(x)\u001b[38;5;241m.\u001b[39mreshape(B, N, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m)\n\u001b[1;32m     90\u001b[0m     q, k, v \u001b[38;5;241m=\u001b[39m qkv\u001b[38;5;241m.\u001b[39munbind(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     91\u001b[0m     q, k \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mq_norm(q), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk_norm(k)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.13/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# --- VERIFY FIX: Run AFTER re-running DINO class cell (3.5) ---\n",
    "overfit_transform = transforms.Compose([\n",
    "    Rescale((224, 224)), NormalizeOriginal(), ToTensor()\n",
    "])\n",
    "overfit_dataset = FacialKeypointsDataset(\n",
    "    'data/training_frames_keypoints.csv', 'data/training', overfit_transform\n",
    ")\n",
    "overfit_subset = torch.utils.data.Subset(overfit_dataset, range(32))\n",
    "batch = next(iter(DataLoader(overfit_subset, batch_size=32, shuffle=False)))\n",
    "images = batch['image'].to(device)\n",
    "keypoints = batch['keypoints'].reshape(32, 136).float().to(device)\n",
    "\n",
    "mean_mse = ((keypoints - keypoints.mean(0, keepdim=True)) ** 2).mean().item()\n",
    "print(f\"Mean-baseline MSE: {mean_mse:.6f}\\n\")\n",
    "\n",
    "# Full model overfit test (ImageNet norm + CLS token + BatchNorm + MLP)\n",
    "dino_overfit = DINOKeypointDetector(pretrained=True, freeze_backbone=True).to(device)\n",
    "\n",
    "# Disable dropout for overfit test\n",
    "for m in dino_overfit.regression_head.modules():\n",
    "    if isinstance(m, nn.Dropout):\n",
    "        m.p = 0.0\n",
    "\n",
    "dino_overfit.train()  # backbone stays eval (via override), BN/MLP in train mode\n",
    "opt = torch.optim.Adam([p for p in dino_overfit.parameters() if p.requires_grad], lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "losses = []\n",
    "for step in range(1, 501):\n",
    "    preds = dino_overfit(images)\n",
    "    loss = loss_fn(preds, keypoints)\n",
    "    opt.zero_grad(); loss.backward(); opt.step()\n",
    "    losses.append(loss.item())\n",
    "    if step in [1, 50, 100, 200, 300, 500]:\n",
    "        print(f\"Step {step:3d}: MSE = {loss.item():.6f}\")\n",
    "\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Step'); plt.ylabel('MSE'); plt.yscale('log')\n",
    "plt.title('DINO overfit: ImageNet norm + CLS + BatchNorm1d + MLP')\n",
    "plt.grid(True); plt.show()\n",
    "\n",
    "print(f\"\\nFinal MSE: {losses[-1]:.6f}  (mean-baseline: {mean_mse:.6f})\")\n",
    "if losses[-1] < 1e-3:\n",
    "    print(\"PASS! Pipeline works. Re-run DINO training cells.\")\n",
    "else:\n",
    "    print(f\"Improvement: {mean_mse/losses[-1]:.1f}x better than mean-baseline\")\n",
    "\n",
    "del dino_overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. U-Net (Heatmap-based)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    \"\"\"Two consecutive conv-bn-relu blocks.\"\"\"\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_keypoints=68, heatmap_size=64):\n",
    "        super().__init__()\n",
    "        self.heatmap_size = heatmap_size\n",
    "\n",
    "        # Encoder\n",
    "        self.enc1 = DoubleConv(in_channels, 32)\n",
    "        self.enc2 = DoubleConv(32, 64)\n",
    "        self.enc3 = DoubleConv(64, 128)\n",
    "        self.enc4 = DoubleConv(128, 256)\n",
    "        self.pool = nn.MaxPool2d(2)\n",
    "        self.dropout_enc = nn.Dropout2d(p=0.3)\n",
    "\n",
    "        # Bottleneck\n",
    "        self.bottleneck = DoubleConv(256, 512)\n",
    "        self.dropout_bottleneck = nn.Dropout2d(p=0.3)\n",
    "        self.dropout_dec = nn.Dropout2d(p=0.2)\n",
    "\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.dec4 = DoubleConv(512, 256)\n",
    "        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.dec3 = DoubleConv(256, 128)\n",
    "        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.dec2 = DoubleConv(128, 64)\n",
    "        self.up1 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n",
    "        self.dec1 = DoubleConv(64, 32)\n",
    "\n",
    "        self.out_conv = nn.Conv2d(32, num_keypoints, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)\n",
    "        e2 = self.enc2(self.pool(e1))\n",
    "        e3 = self.enc3(self.pool(e2))\n",
    "        e4 = self.dropout_enc(self.enc4(self.pool(e3)))\n",
    "\n",
    "        b = self.dropout_bottleneck(self.bottleneck(self.pool(e4)))\n",
    "\n",
    "        d4 = self.dropout_dec(self.dec4(torch.cat([self.up4(b), e4], dim=1)))\n",
    "        d3 = self.dropout_dec(self.dec3(torch.cat([self.up3(d4), e3], dim=1)))\n",
    "        d2 = self.dec2(torch.cat([self.up2(d3), e2], dim=1))\n",
    "        d1 = self.dec1(torch.cat([self.up1(d2), e1], dim=1))\n",
    "\n",
    "        out = self.out_conv(d1)\n",
    "        out = F.interpolate(out, size=self.heatmap_size, mode='bilinear', align_corners=False)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Train U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# KEY FIXES vs original:\n",
    "#   1. batch_size 256->32 (gives ~90 steps/epoch instead of ~11)\n",
    "#   2. MSE loss instead of BCE (BCE+pos_weight causes vertical stripe collapse)\n",
    "#   3. Cosine annealing scheduler (smooth LR decay)\n",
    "#   4. weight_decay 1e-4 (stronger regularization)\n",
    "UNET_EPOCHS = 50\n",
    "UNET_LR = 3e-4\n",
    "UNET_BATCH_SIZE = 32\n",
    "UNET_LOSS = 'mse'           # 'mse' or 'bce'\n",
    "UNET_SCHEDULER = 'cosine'   # 'cosine' or 'plateau'\n",
    "\n",
    "# --- Initialize ---\n",
    "unet_model = UNet(in_channels=1, num_keypoints=68, heatmap_size=64).to(device)\n",
    "unet_train_loader, unet_test_loader = load_heatmap_data(batch_size=UNET_BATCH_SIZE)\n",
    "unet_optimizer = torch.optim.Adam(unet_model.parameters(), lr=UNET_LR, weight_decay=1e-4)\n",
    "\n",
    "print(f\"U-Net parameters: {sum(p.numel() for p in unet_model.parameters()):,}\")\n",
    "print(f\"Batches per epoch: {len(unet_train_loader)}\")\n",
    "print(f\"Total steps: {UNET_EPOCHS * len(unet_train_loader)}\")\n",
    "print(f\"Loss: {UNET_LOSS}, Scheduler: {UNET_SCHEDULER}\")\n",
    "\n",
    "# --- Train ---\n",
    "wandb.init(project='facial-keypoints', name=f'unet-{UNET_LOSS}', reinit=True)\n",
    "wandb.config.update({'model': 'unet', 'criterion': UNET_LOSS, 'lr': UNET_LR,\n",
    "                      'batch_size': UNET_BATCH_SIZE, 'scheduler': UNET_SCHEDULER})\n",
    "\n",
    "unet_epoch, unet_step = train_heatmap(\n",
    "    unet_model, unet_train_loader, unet_test_loader, unet_optimizer,\n",
    "    device, model_name='unet', num_epochs=UNET_EPOCHS,\n",
    "    loss_type=UNET_LOSS, scheduler_type=UNET_SCHEDULER\n",
    ")\n",
    "\n",
    "save_checkpoint(unet_model, unet_optimizer, unet_epoch, unet_step, 'unet',\n",
    "                path='checkpoints-unet/last_checkpoint.pth')\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Visualize U-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_keypoints(unet_test_loader, unet_model, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 DEBUG: U-Net Post-Training Diagnostics\n",
    "Run this after training to diagnose **overfitting**, **keypoint collapse**, and **heatmap quality**.\n",
    "Symptoms: val loss plateaus at ~0.2, train keeps improving, predictions cluster to a few face points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================================\n",
    "# U-NET POST-TRAINING DIAGNOSTICS\n",
    "# =====================================================================\n",
    "# Requires: unet_model (trained), unet_test_loader already defined\n",
    "\n",
    "unet_model.eval()\n",
    "\n",
    "# Use deterministic test loader for diagnostics\n",
    "diag_transform = transforms.Compose([Rescale((224, 224)), NormalizeOriginal(), ToTensor()])\n",
    "diag_dataset = FacialKeypointsHeatmapDataset(\n",
    "    'data/test_frames_keypoints.csv', 'data/test',\n",
    "    transform=diag_transform, output_size=64, sigma=2, image_size=224)\n",
    "diag_loader = DataLoader(diag_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ---- 1. Collect predictions on full test set ----\n",
    "print(\"=\" * 60)\n",
    "print(\"1. HEATMAP & KEYPOINT STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_pred_peaks = []\n",
    "all_pixel_errors = []\n",
    "all_pred_kpts_px = []\n",
    "all_gt_kpts_px = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in diag_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        logits = unet_model(images)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Peak confidence per keypoint\n",
    "        peaks = probs.view(probs.size(0), 68, -1).max(dim=-1)[0]\n",
    "        all_pred_peaks.append(peaks.cpu())\n",
    "\n",
    "        # Keypoint coords and errors\n",
    "        pred_kpts = heatmaps_to_keypoints(logits).cpu()\n",
    "        gt_kpts = batch['keypoints']\n",
    "        pred_px = pred_kpts * 50 + 100\n",
    "        gt_px = gt_kpts * 50 + 100\n",
    "        errors = torch.sqrt(((pred_px - gt_px) ** 2).sum(dim=-1))\n",
    "\n",
    "        all_pixel_errors.append(errors)\n",
    "        all_pred_kpts_px.append(pred_px)\n",
    "        all_gt_kpts_px.append(gt_px)\n",
    "\n",
    "pred_peaks = torch.cat(all_pred_peaks, dim=0)      # (N, 68)\n",
    "pixel_errors = torch.cat(all_pixel_errors, dim=0)   # (N, 68)\n",
    "pred_kpts_px = torch.cat(all_pred_kpts_px, dim=0)   # (N, 68, 2)\n",
    "gt_kpts_px = torch.cat(all_gt_kpts_px, dim=0)       # (N, 68, 2)\n",
    "\n",
    "print(f\"Test samples analyzed: {pred_peaks.size(0)}\")\n",
    "print(f\"\\nPredicted heatmap peak confidence:\")\n",
    "print(f\"  Mean: {pred_peaks.mean():.4f}, Std: {pred_peaks.std():.4f}\")\n",
    "print(f\"  Min across keypoints: {pred_peaks.mean(0).min():.4f} (kpt {pred_peaks.mean(0).argmin().item()})\")\n",
    "print(f\"  Max across keypoints: {pred_peaks.mean(0).max():.4f} (kpt {pred_peaks.mean(0).argmax().item()})\")\n",
    "print(f\"\\nPixel error (test set):\")\n",
    "print(f\"  Mean:   {pixel_errors.mean():.2f} px\")\n",
    "print(f\"  Median: {pixel_errors.median():.2f} px\")\n",
    "print(f\"  90th %: {pixel_errors.quantile(0.9):.2f} px\")\n",
    "\n",
    "# NME (Normalized Mean Error) — normalize by inter-ocular distance\n",
    "# Keypoints 36-41 = left eye, 42-47 = right eye\n",
    "left_eye_center = gt_kpts_px[:, 36:42, :].mean(dim=1)   # (N, 2)\n",
    "right_eye_center = gt_kpts_px[:, 42:48, :].mean(dim=1)\n",
    "iod = torch.sqrt(((left_eye_center - right_eye_center) ** 2).sum(dim=-1))  # (N,)\n",
    "nme = (pixel_errors.mean(dim=1) / iod).mean().item()\n",
    "print(f\"  NME (inter-ocular): {nme:.4f} ({nme*100:.2f}%)\")\n",
    "\n",
    "# ---- 2. Keypoint collapse detection ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. KEYPOINT COLLAPSE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For each sample, measure the spread of predicted keypoints\n",
    "pred_spread_x = pred_kpts_px[:, :, 0].std(dim=1)  # (N,)\n",
    "pred_spread_y = pred_kpts_px[:, :, 1].std(dim=1)\n",
    "gt_spread_x = gt_kpts_px[:, :, 0].std(dim=1)\n",
    "gt_spread_y = gt_kpts_px[:, :, 1].std(dim=1)\n",
    "\n",
    "print(f\"Predicted keypoint spread (std across 68 kpts):\")\n",
    "print(f\"  X: {pred_spread_x.mean():.1f} px (GT: {gt_spread_x.mean():.1f} px)\")\n",
    "print(f\"  Y: {pred_spread_y.mean():.1f} px (GT: {gt_spread_y.mean():.1f} px)\")\n",
    "print(f\"  Ratio (pred/GT): X={pred_spread_x.mean()/gt_spread_x.mean():.2f}, Y={pred_spread_y.mean()/gt_spread_y.mean():.2f}\")\n",
    "\n",
    "if pred_spread_x.mean() < gt_spread_x.mean() * 0.5 or pred_spread_y.mean() < gt_spread_y.mean() * 0.5:\n",
    "    print(\"  >>> KEYPOINT COLLAPSE DETECTED: predictions cluster too tightly!\")\n",
    "    print(\"      Model is predicting ~same location for many keypoints.\")\n",
    "else:\n",
    "    print(\"  Spread looks reasonable (no collapse).\")\n",
    "\n",
    "# Check per-sample: how many unique argmax positions?\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(diag_loader))\n",
    "    sample_logits = unet_model(sample_batch['image'][:4].to(device))\n",
    "    sample_flat = sample_logits.view(4, 68, -1)\n",
    "    sample_argmax = sample_flat.argmax(dim=-1)  # (4, 68)\n",
    "\n",
    "for s_idx in range(4):\n",
    "    unique_positions = sample_argmax[s_idx].unique().numel()\n",
    "    print(f\"  Sample {s_idx}: {unique_positions}/68 unique argmax positions\", end=\"\")\n",
    "    if unique_positions < 40:\n",
    "        print(\" << MANY KEYPOINTS SHARE SAME PEAK!\")\n",
    "    else:\n",
    "        print(\" (OK)\")\n",
    "\n",
    "# ---- 3. Visual: summed heatmaps (collapse = single blob) ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. HEATMAP VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    vis_batch = next(iter(diag_loader))\n",
    "    vis_images = vis_batch['image'][:4].to(device)\n",
    "    vis_logits = unet_model(vis_images)\n",
    "    vis_probs = torch.sigmoid(vis_logits).cpu()\n",
    "    vis_gt = vis_batch['heatmaps'][:4]\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "for col in range(4):\n",
    "    img = vis_images[col, 0].cpu().numpy()\n",
    "    summed_pred = vis_probs[col].sum(dim=0).numpy()\n",
    "    summed_gt = vis_gt[col].sum(dim=0).numpy()\n",
    "\n",
    "    axes[0, col].imshow(img, cmap='gray')\n",
    "    axes[0, col].set_title(f'Input {col}')\n",
    "    axes[0, col].axis('off')\n",
    "\n",
    "    axes[1, col].imshow(summed_pred, cmap='hot')\n",
    "    axes[1, col].set_title(f'Sum pred (max={summed_pred.max():.1f})')\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "    axes[2, col].imshow(summed_gt, cmap='hot')\n",
    "    axes[2, col].set_title(f'Sum GT (max={summed_gt.max():.1f})')\n",
    "    axes[2, col].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Image', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Pred (sum 68)', fontsize=12)\n",
    "axes[2, 0].set_ylabel('GT (sum 68)', fontsize=12)\n",
    "plt.suptitle('Collapse check: summed pred should spread like summed GT\\n'\n",
    "             '(If pred is a single blob = keypoint collapse)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 4. Per-keypoint heatmaps for a single sample ----\n",
    "# Show 10 representative keypoints: pred vs GT side by side\n",
    "fig, axes = plt.subplots(2, 10, figsize=(25, 5))\n",
    "kpt_indices = [0, 8, 16, 27, 30, 33, 36, 42, 48, 57]  # jawline, brow, nose, eyes, mouth\n",
    "for col_idx, kpt_idx in enumerate(kpt_indices):\n",
    "    axes[0, col_idx].imshow(vis_gt[0, kpt_idx].numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "    axes[0, col_idx].set_title(f'GT kpt {kpt_idx}', fontsize=8)\n",
    "    axes[0, col_idx].axis('off')\n",
    "    axes[1, col_idx].imshow(vis_probs[0, kpt_idx].numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "    axes[1, col_idx].set_title(f'Pred kpt {kpt_idx}', fontsize=8)\n",
    "    axes[1, col_idx].axis('off')\n",
    "axes[0, 0].set_ylabel('GT', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Pred', fontsize=11)\n",
    "plt.suptitle('Individual keypoint heatmaps (sample 0) — pred should match GT location', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 5. Per-keypoint error bar chart ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. PER-KEYPOINT ERROR BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mean_per_kpt = pixel_errors.mean(dim=0)  # (68,)\n",
    "worst_5 = mean_per_kpt.argsort(descending=True)[:5]\n",
    "best_5 = mean_per_kpt.argsort()[:5]\n",
    "\n",
    "kpt_names = {0: 'jaw-R', 8: 'chin', 16: 'jaw-L', 17: 'brow-R-out', 21: 'brow-R-in',\n",
    "             22: 'brow-L-in', 26: 'brow-L-out', 27: 'nose-top', 30: 'nose-tip',\n",
    "             36: 'eye-R-out', 39: 'eye-R-in', 42: 'eye-L-in', 45: 'eye-L-out',\n",
    "             48: 'mouth-R', 54: 'mouth-L', 51: 'lip-top', 57: 'lip-bottom', 62: 'lip-inner-top'}\n",
    "\n",
    "print(\"Best 5 keypoints:\")\n",
    "for idx in best_5:\n",
    "    name = kpt_names.get(idx.item(), '')\n",
    "    print(f\"  Kpt {idx.item():2d} {name:15s}: {mean_per_kpt[idx]:.2f} px\")\n",
    "print(\"Worst 5 keypoints:\")\n",
    "for idx in worst_5:\n",
    "    name = kpt_names.get(idx.item(), '')\n",
    "    print(f\"  Kpt {idx.item():2d} {name:15s}: {mean_per_kpt[idx]:.2f} px\")\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "colors = ['#e74c3c' if e > mean_per_kpt.mean() + mean_per_kpt.std() else '#3498db'\n",
    "          for e in mean_per_kpt]\n",
    "plt.bar(range(68), mean_per_kpt.numpy(), color=colors)\n",
    "plt.axhline(y=mean_per_kpt.mean(), color='k', linestyle='--', alpha=0.5, label=f'Mean={mean_per_kpt.mean():.1f}px')\n",
    "plt.xlabel('Keypoint index')\n",
    "plt.ylabel('Mean pixel error')\n",
    "plt.title('Per-keypoint error on test set (red = >1 std above mean)')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 6. Overlay predictions on face ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. PREDICTION OVERLAY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "vis_pred_kpts = heatmaps_to_keypoints(vis_logits.cpu()).numpy()\n",
    "vis_gt_kpts = vis_batch['keypoints'][:4].numpy()\n",
    "\n",
    "for i in range(4):\n",
    "    img = vis_images[i, 0].cpu().numpy()\n",
    "    pred = vis_pred_kpts[i] * 50 + 100\n",
    "    gt = vis_gt_kpts[i] * 50 + 100\n",
    "\n",
    "    axes[0, i].imshow(img, cmap='gray')\n",
    "    axes[0, i].scatter(pred[:, 0], pred[:, 1], c='r', s=15, alpha=0.8, zorder=5)\n",
    "    axes[0, i].scatter(gt[:, 0], gt[:, 1], c='g', s=15, alpha=0.5, marker='x', zorder=4)\n",
    "    axes[0, i].set_title(f'Sample {i}: Pred(red) vs GT(green)')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Per-keypoint error on this sample\n",
    "    errs = np.sqrt(((pred - gt) ** 2).sum(axis=-1))\n",
    "    axes[1, i].imshow(img, cmap='gray')\n",
    "    sc = axes[1, i].scatter(pred[:, 0], pred[:, 1], c=errs, cmap='RdYlGn_r',\n",
    "                            s=25, vmin=0, vmax=30, zorder=5)\n",
    "    # Draw lines from pred to GT for worst errors\n",
    "    worst_10 = errs.argsort()[-10:]\n",
    "    for w in worst_10:\n",
    "        axes[1, i].plot([pred[w, 0], gt[w, 0]], [pred[w, 1], gt[w, 1]], 'w-', alpha=0.6, linewidth=0.8)\n",
    "    axes[1, i].set_title(f'Error map (mean={errs.mean():.1f}px)')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.colorbar(sc, ax=axes[1, :], label='Pixel error', shrink=0.8)\n",
    "plt.suptitle('Top: predictions overlay | Bottom: error heatmap (white lines = worst 10)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 7. Data augmentation sanity check ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. AUGMENTATION SANITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Checking that augmented training samples have aligned heatmaps...\")\n",
    "\n",
    "train_transform_debug = transforms.Compose([\n",
    "    Rescale(250), RandomCrop(224), RandomHorizontalFlip(), RandomRotate(15),\n",
    "    ColorJitter(), NormalizeOriginal(), ToTensor()\n",
    "])\n",
    "debug_hm_dataset = FacialKeypointsHeatmapDataset(\n",
    "    'data/training_frames_keypoints.csv', 'data/training',\n",
    "    transform=train_transform_debug, output_size=64, sigma=2, image_size=224)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 5))\n",
    "for col in range(4):\n",
    "    sample = debug_hm_dataset[col]\n",
    "    img = sample['image'][0].numpy()  # (224, 224)\n",
    "    hm_sum = sample['heatmaps'].sum(dim=0).numpy()  # (64, 64)\n",
    "    kpts = sample['keypoints'].numpy() * 50 + 100  # (68, 2) in pixel space\n",
    "\n",
    "    axes[0, col].imshow(img, cmap='gray')\n",
    "    axes[0, col].scatter(kpts[:, 0], kpts[:, 1], c='lime', s=8, alpha=0.7)\n",
    "    axes[0, col].set_title(f'Aug sample {col}')\n",
    "    axes[0, col].axis('off')\n",
    "\n",
    "    axes[1, col].imshow(hm_sum, cmap='hot')\n",
    "    # Overlay expected keypoint positions on heatmap\n",
    "    kpts_hm = kpts * (64 / 224)\n",
    "    axes[1, col].scatter(kpts_hm[:, 0], kpts_hm[:, 1], c='lime', s=8, alpha=0.5)\n",
    "    axes[1, col].set_title(f'Summed heatmap + kpts')\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "plt.suptitle('Augmentation check: green dots should align with heatmap peaks\\n'\n",
    "             '(Misalignment = augmentation bug)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- DIAGNOSIS SUMMARY ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIAGNOSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mean_err = pixel_errors.mean().item()\n",
    "spread_ratio_x = pred_spread_x.mean() / gt_spread_x.mean()\n",
    "spread_ratio_y = pred_spread_y.mean() / gt_spread_y.mean()\n",
    "peak_conf = pred_peaks.mean().item()\n",
    "\n",
    "issues = []\n",
    "if spread_ratio_x < 0.5 or spread_ratio_y < 0.5:\n",
    "    issues.append(f\"KEYPOINT COLLAPSE: spread ratio = ({spread_ratio_x:.2f}x, {spread_ratio_y:.2f}y)\")\n",
    "if peak_conf < 0.3:\n",
    "    issues.append(f\"LOW CONFIDENCE: peak = {peak_conf:.3f} (heatmaps are too diffuse)\")\n",
    "if peak_conf > 0.95:\n",
    "    issues.append(f\"OVERCONFIDENT: peak = {peak_conf:.3f} (may be memorizing noise)\")\n",
    "if mean_err > 15:\n",
    "    issues.append(f\"HIGH ERROR: {mean_err:.1f} px mean (poor generalization)\")\n",
    "if nme > 0.10:\n",
    "    issues.append(f\"HIGH NME: {nme*100:.2f}% (>10% is poor)\")\n",
    "\n",
    "if issues:\n",
    "    print(\"Issues found:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "    print(\"\\nRecommended fixes (try in order):\")\n",
    "    print(\"  1. SMALLER BATCH SIZE: 256 -> 32 or 64 (more gradient updates/epoch)\")\n",
    "    print(\"  2. STRONGER REGULARIZATION: increase dropout 0.3->0.5, weight_decay 1e-5->1e-4\")\n",
    "    print(\"  3. MSE LOSS: replace BCE with MSE (less prone to false-positive activation)\")\n",
    "    print(\"  4. LOWER LR: start at 3e-4 with cosine annealing to 1e-6\")\n",
    "    print(\"  5. FEWER CHANNELS: reduce encoder to 16-32-64-128 (less overfitting)\")\n",
    "    print(\"  6. LARGER SIGMA: sigma=2->3 in heatmap generation (easier targets)\")\n",
    "    print(\"  7. EARLY STOPPING: save best val_loss model, stop if no improvement for 15 epochs\")\n",
    "else:\n",
    "    print(f\"Model looks healthy! Mean error: {mean_err:.1f} px, NME: {nme*100:.2f}%, Peak: {peak_conf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 DEBUG: U-Net 8-Sample Overfit Test\n",
    "If the U-Net can't overfit a tiny batch of heatmaps, there's a bug in the heatmap generation, model architecture, or loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =====================================================================\n",
    "# U-NET POST-TRAINING DIAGNOSTICS\n",
    "# =====================================================================\n",
    "# Requires: unet_model (trained), unet_test_loader already defined\n",
    "\n",
    "unet_model.eval()\n",
    "\n",
    "# Use deterministic test loader for diagnostics\n",
    "diag_transform = transforms.Compose([Rescale((224, 224)), NormalizeOriginal(), ToTensor()])\n",
    "diag_dataset = FacialKeypointsHeatmapDataset(\n",
    "    'data/test_frames_keypoints.csv', 'data/test',\n",
    "    transform=diag_transform, output_size=64, sigma=2, image_size=224)\n",
    "diag_loader = DataLoader(diag_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# ---- 1. Collect predictions on full test set ----\n",
    "print(\"=\" * 60)\n",
    "print(\"1. HEATMAP & KEYPOINT STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "all_pred_peaks = []\n",
    "all_pixel_errors = []\n",
    "all_pred_kpts_px = []\n",
    "all_gt_kpts_px = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in diag_loader:\n",
    "        images = batch['image'].to(device)\n",
    "        logits = unet_model(images)\n",
    "        probs = torch.sigmoid(logits)\n",
    "\n",
    "        # Peak confidence per keypoint\n",
    "        peaks = probs.view(probs.size(0), 68, -1).max(dim=-1)[0]\n",
    "        all_pred_peaks.append(peaks.cpu())\n",
    "\n",
    "        # Keypoint coords and errors\n",
    "        pred_kpts = heatmaps_to_keypoints(logits).cpu()\n",
    "        gt_kpts = batch['keypoints']\n",
    "        pred_px = pred_kpts * 50 + 100\n",
    "        gt_px = gt_kpts * 50 + 100\n",
    "        errors = torch.sqrt(((pred_px - gt_px) ** 2).sum(dim=-1))\n",
    "\n",
    "        all_pixel_errors.append(errors)\n",
    "        all_pred_kpts_px.append(pred_px)\n",
    "        all_gt_kpts_px.append(gt_px)\n",
    "\n",
    "pred_peaks = torch.cat(all_pred_peaks, dim=0)      # (N, 68)\n",
    "pixel_errors = torch.cat(all_pixel_errors, dim=0)   # (N, 68)\n",
    "pred_kpts_px = torch.cat(all_pred_kpts_px, dim=0)   # (N, 68, 2)\n",
    "gt_kpts_px = torch.cat(all_gt_kpts_px, dim=0)       # (N, 68, 2)\n",
    "\n",
    "print(f\"Test samples analyzed: {pred_peaks.size(0)}\")\n",
    "print(f\"\\nPredicted heatmap peak confidence:\")\n",
    "print(f\"  Mean: {pred_peaks.mean():.4f}, Std: {pred_peaks.std():.4f}\")\n",
    "print(f\"  Min across keypoints: {pred_peaks.mean(0).min():.4f} (kpt {pred_peaks.mean(0).argmin().item()})\")\n",
    "print(f\"  Max across keypoints: {pred_peaks.mean(0).max():.4f} (kpt {pred_peaks.mean(0).argmax().item()})\")\n",
    "print(f\"\\nPixel error (test set):\")\n",
    "print(f\"  Mean:   {pixel_errors.mean():.2f} px\")\n",
    "print(f\"  Median: {pixel_errors.median():.2f} px\")\n",
    "print(f\"  90th %: {pixel_errors.quantile(0.9):.2f} px\")\n",
    "\n",
    "# NME (Normalized Mean Error) — normalize by inter-ocular distance\n",
    "# Keypoints 36-41 = left eye, 42-47 = right eye\n",
    "left_eye_center = gt_kpts_px[:, 36:42, :].mean(dim=1)   # (N, 2)\n",
    "right_eye_center = gt_kpts_px[:, 42:48, :].mean(dim=1)\n",
    "iod = torch.sqrt(((left_eye_center - right_eye_center) ** 2).sum(dim=-1))  # (N,)\n",
    "nme = (pixel_errors.mean(dim=1) / iod).mean().item()\n",
    "print(f\"  NME (inter-ocular): {nme:.4f} ({nme*100:.2f}%)\")\n",
    "\n",
    "# ---- 2. Keypoint collapse detection ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. KEYPOINT COLLAPSE DETECTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# For each sample, measure the spread of predicted keypoints\n",
    "pred_spread_x = pred_kpts_px[:, :, 0].std(dim=1)  # (N,)\n",
    "pred_spread_y = pred_kpts_px[:, :, 1].std(dim=1)\n",
    "gt_spread_x = gt_kpts_px[:, :, 0].std(dim=1)\n",
    "gt_spread_y = gt_kpts_px[:, :, 1].std(dim=1)\n",
    "\n",
    "print(f\"Predicted keypoint spread (std across 68 kpts):\")\n",
    "print(f\"  X: {pred_spread_x.mean():.1f} px (GT: {gt_spread_x.mean():.1f} px)\")\n",
    "print(f\"  Y: {pred_spread_y.mean():.1f} px (GT: {gt_spread_y.mean():.1f} px)\")\n",
    "print(f\"  Ratio (pred/GT): X={pred_spread_x.mean()/gt_spread_x.mean():.2f}, Y={pred_spread_y.mean()/gt_spread_y.mean():.2f}\")\n",
    "\n",
    "if pred_spread_x.mean() < gt_spread_x.mean() * 0.5 or pred_spread_y.mean() < gt_spread_y.mean() * 0.5:\n",
    "    print(\"  >>> KEYPOINT COLLAPSE DETECTED: predictions cluster too tightly!\")\n",
    "    print(\"      Model is predicting ~same location for many keypoints.\")\n",
    "else:\n",
    "    print(\"  Spread looks reasonable (no collapse).\")\n",
    "\n",
    "# Check per-sample: how many unique argmax positions?\n",
    "with torch.no_grad():\n",
    "    sample_batch = next(iter(diag_loader))\n",
    "    sample_logits = unet_model(sample_batch['image'][:4].to(device))\n",
    "    sample_flat = sample_logits.view(4, 68, -1)\n",
    "    sample_argmax = sample_flat.argmax(dim=-1)  # (4, 68)\n",
    "\n",
    "for s_idx in range(4):\n",
    "    unique_positions = sample_argmax[s_idx].unique().numel()\n",
    "    print(f\"  Sample {s_idx}: {unique_positions}/68 unique argmax positions\", end=\"\")\n",
    "    if unique_positions < 40:\n",
    "        print(\" << MANY KEYPOINTS SHARE SAME PEAK!\")\n",
    "    else:\n",
    "        print(\" (OK)\")\n",
    "\n",
    "# ---- 3. Visual: summed heatmaps (collapse = single blob) ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. HEATMAP VISUALIZATIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "with torch.no_grad():\n",
    "    vis_batch = next(iter(diag_loader))\n",
    "    vis_images = vis_batch['image'][:4].to(device)\n",
    "    vis_logits = unet_model(vis_images)\n",
    "    vis_probs = torch.sigmoid(vis_logits).cpu()\n",
    "    vis_gt = vis_batch['heatmaps'][:4]\n",
    "\n",
    "fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n",
    "for col in range(4):\n",
    "    img = vis_images[col, 0].cpu().numpy()\n",
    "    summed_pred = vis_probs[col].sum(dim=0).numpy()\n",
    "    summed_gt = vis_gt[col].sum(dim=0).numpy()\n",
    "\n",
    "    axes[0, col].imshow(img, cmap='gray')\n",
    "    axes[0, col].set_title(f'Input {col}')\n",
    "    axes[0, col].axis('off')\n",
    "\n",
    "    axes[1, col].imshow(summed_pred, cmap='hot')\n",
    "    axes[1, col].set_title(f'Sum pred (max={summed_pred.max():.1f})')\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "    axes[2, col].imshow(summed_gt, cmap='hot')\n",
    "    axes[2, col].set_title(f'Sum GT (max={summed_gt.max():.1f})')\n",
    "    axes[2, col].axis('off')\n",
    "\n",
    "axes[0, 0].set_ylabel('Image', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Pred (sum 68)', fontsize=12)\n",
    "axes[2, 0].set_ylabel('GT (sum 68)', fontsize=12)\n",
    "plt.suptitle('Collapse check: summed pred should spread like summed GT\\n'\n",
    "             '(If pred is a single blob = keypoint collapse)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 4. Per-keypoint heatmaps for a single sample ----\n",
    "# Show 10 representative keypoints: pred vs GT side by side\n",
    "fig, axes = plt.subplots(2, 10, figsize=(25, 5))\n",
    "kpt_indices = [0, 8, 16, 27, 30, 33, 36, 42, 48, 57]  # jawline, brow, nose, eyes, mouth\n",
    "for col_idx, kpt_idx in enumerate(kpt_indices):\n",
    "    axes[0, col_idx].imshow(vis_gt[0, kpt_idx].numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "    axes[0, col_idx].set_title(f'GT kpt {kpt_idx}', fontsize=8)\n",
    "    axes[0, col_idx].axis('off')\n",
    "    axes[1, col_idx].imshow(vis_probs[0, kpt_idx].numpy(), cmap='hot', vmin=0, vmax=1)\n",
    "    axes[1, col_idx].set_title(f'Pred kpt {kpt_idx}', fontsize=8)\n",
    "    axes[1, col_idx].axis('off')\n",
    "axes[0, 0].set_ylabel('GT', fontsize=11)\n",
    "axes[1, 0].set_ylabel('Pred', fontsize=11)\n",
    "plt.suptitle('Individual keypoint heatmaps (sample 0) — pred should match GT location', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 5. Per-keypoint error bar chart ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. PER-KEYPOINT ERROR BREAKDOWN\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mean_per_kpt = pixel_errors.mean(dim=0)  # (68,)\n",
    "worst_5 = mean_per_kpt.argsort(descending=True)[:5]\n",
    "best_5 = mean_per_kpt.argsort()[:5]\n",
    "\n",
    "kpt_names = {0: 'jaw-R', 8: 'chin', 16: 'jaw-L', 17: 'brow-R-out', 21: 'brow-R-in',\n",
    "             22: 'brow-L-in', 26: 'brow-L-out', 27: 'nose-top', 30: 'nose-tip',\n",
    "             36: 'eye-R-out', 39: 'eye-R-in', 42: 'eye-L-in', 45: 'eye-L-out',\n",
    "             48: 'mouth-R', 54: 'mouth-L', 51: 'lip-top', 57: 'lip-bottom', 62: 'lip-inner-top'}\n",
    "\n",
    "print(\"Best 5 keypoints:\")\n",
    "for idx in best_5:\n",
    "    name = kpt_names.get(idx.item(), '')\n",
    "    print(f\"  Kpt {idx.item():2d} {name:15s}: {mean_per_kpt[idx]:.2f} px\")\n",
    "print(\"Worst 5 keypoints:\")\n",
    "for idx in worst_5:\n",
    "    name = kpt_names.get(idx.item(), '')\n",
    "    print(f\"  Kpt {idx.item():2d} {name:15s}: {mean_per_kpt[idx]:.2f} px\")\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "colors = ['#e74c3c' if e > mean_per_kpt.mean() + mean_per_kpt.std() else '#3498db'\n",
    "          for e in mean_per_kpt]\n",
    "plt.bar(range(68), mean_per_kpt.numpy(), color=colors)\n",
    "plt.axhline(y=mean_per_kpt.mean(), color='k', linestyle='--', alpha=0.5, label=f'Mean={mean_per_kpt.mean():.1f}px')\n",
    "plt.xlabel('Keypoint index')\n",
    "plt.ylabel('Mean pixel error')\n",
    "plt.title('Per-keypoint error on test set (red = >1 std above mean)')\n",
    "plt.legend()\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 6. Overlay predictions on face ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. PREDICTION OVERLAY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "vis_pred_kpts = heatmaps_to_keypoints(vis_logits.cpu()).numpy()\n",
    "vis_gt_kpts = vis_batch['keypoints'][:4].numpy()\n",
    "\n",
    "for i in range(4):\n",
    "    img = vis_images[i, 0].cpu().numpy()\n",
    "    pred = vis_pred_kpts[i] * 50 + 100\n",
    "    gt = vis_gt_kpts[i] * 50 + 100\n",
    "\n",
    "    axes[0, i].imshow(img, cmap='gray')\n",
    "    axes[0, i].scatter(pred[:, 0], pred[:, 1], c='r', s=15, alpha=0.8, zorder=5)\n",
    "    axes[0, i].scatter(gt[:, 0], gt[:, 1], c='g', s=15, alpha=0.5, marker='x', zorder=4)\n",
    "    axes[0, i].set_title(f'Sample {i}: Pred(red) vs GT(green)')\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Per-keypoint error on this sample\n",
    "    errs = np.sqrt(((pred - gt) ** 2).sum(axis=-1))\n",
    "    axes[1, i].imshow(img, cmap='gray')\n",
    "    sc = axes[1, i].scatter(pred[:, 0], pred[:, 1], c=errs, cmap='RdYlGn_r',\n",
    "                            s=25, vmin=0, vmax=30, zorder=5)\n",
    "    # Draw lines from pred to GT for worst errors\n",
    "    worst_10 = errs.argsort()[-10:]\n",
    "    for w in worst_10:\n",
    "        axes[1, i].plot([pred[w, 0], gt[w, 0]], [pred[w, 1], gt[w, 1]], 'w-', alpha=0.6, linewidth=0.8)\n",
    "    axes[1, i].set_title(f'Error map (mean={errs.mean():.1f}px)')\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.colorbar(sc, ax=axes[1, :], label='Pixel error', shrink=0.8)\n",
    "plt.suptitle('Top: predictions overlay | Bottom: error heatmap (white lines = worst 10)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- 7. Data augmentation sanity check ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. AUGMENTATION SANITY CHECK\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Checking that augmented training samples have aligned heatmaps...\")\n",
    "\n",
    "train_transform_debug = transforms.Compose([\n",
    "    Rescale(250), RandomCrop(224), RandomHorizontalFlip(), RandomRotate(15),\n",
    "    ColorJitter(), NormalizeOriginal(), ToTensor()\n",
    "])\n",
    "debug_hm_dataset = FacialKeypointsHeatmapDataset(\n",
    "    'data/training_frames_keypoints.csv', 'data/training',\n",
    "    transform=train_transform_debug, output_size=64, sigma=2, image_size=224)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(20, 5))\n",
    "for col in range(4):\n",
    "    sample = debug_hm_dataset[col]\n",
    "    img = sample['image'][0].numpy()  # (224, 224)\n",
    "    hm_sum = sample['heatmaps'].sum(dim=0).numpy()  # (64, 64)\n",
    "    kpts = sample['keypoints'].numpy() * 50 + 100  # (68, 2) in pixel space\n",
    "\n",
    "    axes[0, col].imshow(img, cmap='gray')\n",
    "    axes[0, col].scatter(kpts[:, 0], kpts[:, 1], c='lime', s=8, alpha=0.7)\n",
    "    axes[0, col].set_title(f'Aug sample {col}')\n",
    "    axes[0, col].axis('off')\n",
    "\n",
    "    axes[1, col].imshow(hm_sum, cmap='hot')\n",
    "    # Overlay expected keypoint positions on heatmap\n",
    "    kpts_hm = kpts * (64 / 224)\n",
    "    axes[1, col].scatter(kpts_hm[:, 0], kpts_hm[:, 1], c='lime', s=8, alpha=0.5)\n",
    "    axes[1, col].set_title(f'Summed heatmap + kpts')\n",
    "    axes[1, col].axis('off')\n",
    "\n",
    "plt.suptitle('Augmentation check: green dots should align with heatmap peaks\\n'\n",
    "             '(Misalignment = augmentation bug)', fontsize=13)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ---- DIAGNOSIS SUMMARY ----\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DIAGNOSIS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "mean_err = pixel_errors.mean().item()\n",
    "spread_ratio_x = pred_spread_x.mean() / gt_spread_x.mean()\n",
    "spread_ratio_y = pred_spread_y.mean() / gt_spread_y.mean()\n",
    "peak_conf = pred_peaks.mean().item()\n",
    "\n",
    "issues = []\n",
    "if spread_ratio_x < 0.5 or spread_ratio_y < 0.5:\n",
    "    issues.append(f\"KEYPOINT COLLAPSE: spread ratio = ({spread_ratio_x:.2f}x, {spread_ratio_y:.2f}y)\")\n",
    "if peak_conf < 0.3:\n",
    "    issues.append(f\"LOW CONFIDENCE: peak = {peak_conf:.3f} (heatmaps are too diffuse)\")\n",
    "if peak_conf > 0.95:\n",
    "    issues.append(f\"OVERCONFIDENT: peak = {peak_conf:.3f} (may be memorizing noise)\")\n",
    "if mean_err > 15:\n",
    "    issues.append(f\"HIGH ERROR: {mean_err:.1f} px mean (poor generalization)\")\n",
    "if nme > 0.10:\n",
    "    issues.append(f\"HIGH NME: {nme*100:.2f}% (>10% is poor)\")\n",
    "\n",
    "if issues:\n",
    "    print(\"Issues found:\")\n",
    "    for issue in issues:\n",
    "        print(f\"  - {issue}\")\n",
    "    print(\"\\nRecommended fixes (try in order):\")\n",
    "    print(\"  1. SMALLER BATCH SIZE: 256 -> 32 or 64 (more gradient updates/epoch)\")\n",
    "    print(\"  2. STRONGER REGULARIZATION: increase dropout 0.3->0.5, weight_decay 1e-5->1e-4\")\n",
    "    print(\"  3. MSE LOSS: replace BCE with MSE (less prone to false-positive activation)\")\n",
    "    print(\"  4. LOWER LR: start at 3e-4 with cosine annealing to 1e-6\")\n",
    "    print(\"  5. FEWER CHANNELS: reduce encoder to 16-32-64-128 (less overfitting)\")\n",
    "    print(\"  6. LARGER SIGMA: sigma=2->3 in heatmap generation (easier targets)\")\n",
    "    print(\"  7. EARLY STOPPING: save best val_loss model, stop if no improvement for 15 epochs\")\n",
    "else:\n",
    "    print(f\"Model looks healthy! Mean error: {mean_err:.1f} px, NME: {nme*100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
